from collections.abc import Iterable, Iterator, Mapping, Sequence
from dataclasses import dataclass
from typing import Any, List, NotRequired, Self, cast

import litellm
from django.core.exceptions import ImproperlyConfigured

from ..types import AIResponse, ChatMessage
from .base import (
    BaseChatBackend,
    BaseChatConfig,
    BaseChatConfigSettingsDict,
    BaseConfigSettingsDict,
    BaseEmbeddingBackend,
    BaseEmbeddingConfig,
    BaseEmbeddingConfigSettingsDict,
)


class BaseLiteLLMSettingsDict(BaseConfigSettingsDict):
    DEFAULT_PARAMETERS: NotRequired[Mapping[str, Any] | None]


class LiteLLMBackendSettingsDict(BaseLiteLLMSettingsDict, BaseChatConfigSettingsDict):
    pass


class LiteLLMEmbeddingSettingsDict(
    BaseLiteLLMSettingsDict, BaseEmbeddingConfigSettingsDict
):
    pass


def build_ai_response(response):
    if type(response) == litellm.CustomStreamWrapper:
        return LiteLLMStreamingAIResponse(response)
    return LiteLLMAIResponse(response)


class LiteLLMAIResponse(AIResponse):
    def __init__(self, response: litellm.ModelResponse) -> None:
        self.response = response
        self.choices = cast(litellm.Choices, response.choices)
        self.consumed = False

    def __iter__(self) -> Iterator[str]:
        return self

    def __next__(self):
        if not self.consumed:
            self.consumed = True
            return self.choices[0].message.content
        raise StopIteration

    async def __anext__(self) -> str:
        if not self.consumed:
            self.consumed = True
            return self.choices[0].message.content
        raise StopAsyncIteration


class LiteLLMStreamingAIResponse(AIResponse):
    def __init__(self, response: litellm.CustomStreamWrapper) -> None:
        self.response = response

    def __iter__(self):
        return self

    def __next__(self) -> str:
        next = self.response.__next__()
        if next and next.choices:
            choices = cast(List[litellm.utils.StreamingChoices], next.choices)
            return choices[0].delta.content or ""
        raise StopIteration

    async def __anext__(self) -> str:
        next = await self.response.__anext__()
        if next and next.choices:
            choices = cast(List[litellm.utils.StreamingChoices], next.choices)
            return choices[0].delta.content or ""
        raise StopIteration


@dataclass(kw_only=True)
class LiteLLMBackendConfigMixin:
    default_parameters: Mapping[str, Any]

    @classmethod
    def from_settings(cls, config: BaseLiteLLMSettingsDict, **kwargs: Any) -> Self:
        default_parameters = config.get("DEFAULT_PARAMETERS")
        if default_parameters is None:
            default_parameters = {}
        kwargs.setdefault("default_parameters", default_parameters)

        return super().from_settings(config, **kwargs)  # type: ignore

    @classmethod
    def _get_token_limit(cls, *, model_id: str) -> int:
        """Backend-specific method for retrieving the token limit for the provided model."""
        try:
            model_info = litellm.get_model_info(model=model_id)
            return model_info["max_input_tokens"]
        # We have to catch a generic Exception here because that's what LiteLLM raises
        except Exception as e:
            raise ImproperlyConfigured(
                f"LiteLLM doesn't know about model {model_id}. Set `TOKEN_LIMIT` to specify the maximum tokens accepted by this model as input."
            ) from e


@dataclass(kw_only=True)
class LiteLLMChatBackendConfig(
    LiteLLMBackendConfigMixin, BaseChatConfig[LiteLLMBackendSettingsDict]
):
    pass


@dataclass(kw_only=True)
class LiteLLMEmbeddingBackendConfig(
    LiteLLMBackendConfigMixin, BaseEmbeddingConfig[LiteLLMEmbeddingSettingsDict]
):
    @classmethod
    def _get_embedding_output_dimensions(cls, *, model_id: str) -> int:
        try:
            model_info = litellm.get_model_info(model=model_id)
            return model_info["output_vector_size"]
        # We have to catch a generic Exception here because that's what LiteLLM raises
        except Exception as e:
            raise ImproperlyConfigured(
                f"LiteLLM doesn't know about model {model_id}. Set `EMBEDDING_OUTPUT_DIMENSIONS` to specify the size of the embeddings generated by this model."
            ) from e


class LiteLLMChatBackend(BaseChatBackend[LiteLLMChatBackendConfig]):
    config: LiteLLMChatBackendConfig
    config_cls = LiteLLMChatBackendConfig

    def chat(
        self, *, messages: Sequence[ChatMessage], stream: bool = False, **kwargs
    ) -> AIResponse:
        parameters = {**self.config.default_parameters, **kwargs}
        response = litellm.completion(
            model=self.config.model_id,
            messages=list(messages),
            stream=stream,
            **parameters,
        )
        return build_ai_response(response)

    async def achat(
        self, *, messages: Sequence[ChatMessage], stream: bool = False, **kwargs
    ) -> AIResponse:
        parameters = {**self.config.default_parameters, **kwargs}
        response = await litellm.acompletion(
            model=self.config.model_id,
            messages=list(messages),
            stream=stream,
            **parameters,
        )
        return build_ai_response(response)


class LiteLLMEmbeddingBackend(BaseEmbeddingBackend[LiteLLMEmbeddingBackendConfig]):
    config: LiteLLMEmbeddingBackendConfig
    config_cls = LiteLLMEmbeddingBackendConfig

    def embed(self, inputs: Iterable[str], **kwargs) -> Iterator[list[float]]:
        response = litellm.embedding(
            model=self.config.model_id, inputs=inputs, **kwargs
        )
        yield from [data["embedding"] for data in response["data"]]

    async def aembed(self, inputs: Iterable[str], **kwargs) -> list[float]:
        response = await litellm.aembedding(
            model=self.config.model_id, inputs=inputs, **kwargs
        )

        return [data["embedding"] for data in response["data"]]
